{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taller1_Parte2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeeXWLw75OgE",
        "colab_type": "text"
      },
      "source": [
        "## Pregunta 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kKAV90xJQ2j",
        "colab_type": "code",
        "outputId": "c1d1dbda-007b-4a37-ad57-d3c1b4fab0f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn75i5dAmN-P",
        "colab_type": "code",
        "outputId": "564a45df-3e5a-476e-eada-6bec9ac506fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zKxHJSBkfT2",
        "colab_type": "code",
        "outputId": "a04ed299-e313-4cde-bd7c-2f2d47241f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%time \n",
        "!mkdir train_images\n",
        "!mkdir test_images\n",
        "zip_train_path = '/content/gdrive/My Drive/Colab Notebooks/train_images.zip'\n",
        "zip_test_path = '/content/gdrive/My Drive/Colab Notebooks/test_images.zip'\n",
        "!cp \"{zip_train_path}\" .\n",
        "!cp \"{zip_test_path}\" .\n",
        "!unzip -q train_images.zip -d train_images/\n",
        "!unzip -q test_images.zip -d test_images/\n",
        "!rm train_images.zip\n",
        "!rm test_images.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘train_images’: File exists\n",
            "mkdir: cannot create directory ‘test_images’: File exists\n",
            "cp: cannot stat '/content/gdrive/My Drive/Colab Notebooks/train_images.zip': Transport endpoint is not connected\n",
            "cp: cannot stat '/content/gdrive/My Drive/Colab Notebooks/test_images.zip': Transport endpoint is not connected\n",
            "unzip:  cannot find or open train_images.zip, train_images.zip.zip or train_images.zip.ZIP.\n",
            "unzip:  cannot find or open test_images.zip, test_images.zip.zip or test_images.zip.ZIP.\n",
            "rm: cannot remove 'train_images.zip': No such file or directory\n",
            "rm: cannot remove 'test_images.zip': No such file or directory\n",
            "CPU times: user 69 ms, sys: 39.5 ms, total: 108 ms\n",
            "Wall time: 13.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5trhqOI0RCro",
        "colab_type": "text"
      },
      "source": [
        "### Se importan los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAL_uLf4YeYH",
        "colab_type": "text"
      },
      "source": [
        "Para la división de los datos se utilizo un 80% para datos de entrenamiento y un 20% para el conjunto de test, esto se hizo para elegir un modelo que tengo lo menor posible overfitting en el conjunto  de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zelbZBbdiuW",
        "colab_type": "code",
        "outputId": "55df26b4-27ab-4b6a-ba56-8f25b413face",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dir = '/content/train_images'\n",
        "extension='jpg'\n",
        "\n",
        "paths = []\n",
        "for f in listdir(dir):\n",
        "    if isfile(join(dir,f)):\n",
        "        path = join(dir,f)\n",
        "        number = f.split('_')[1]\n",
        "        number = int(number.split('.')[0])\n",
        "        paths.append((number, path)) \n",
        "paths.sort(key= lambda file: file[0])\n",
        "\n",
        "imgs = []\n",
        "for _, path in paths:\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    image=np.asarray(image)\n",
        "    imgs.append(image)\n",
        "\n",
        "X = np.array(imgs)\n",
        "X = X/127.5 -1.\n",
        "y = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/train_labels.csv')['Expected']\n",
        "\n",
        "classes = np.unique(y)\n",
        "n_classes = len(np.unique(y))\n",
        "count = 0\n",
        "labels ={}\n",
        "for l in classes:\n",
        "    labels[l] = count\n",
        "    count+=1\n",
        "y = np.array(y.replace(labels).tolist())\n",
        "y = to_categorical(y,n_classes)\n",
        "\n",
        "#Split train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40) \n",
        "\n",
        "del X\n",
        "del y\n",
        "del imgs\n",
        "\n",
        "N, width, height, depth = X_train.shape\n",
        "print(f'Cantidad de clases a predecir: {n_classes}')\n",
        "print(f'Clases: {classes}')\n",
        "print(f'Train shape:{X_train.shape}')\n",
        "print(f'Test shape:{X_test.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cantidad de clases a predecir: 10\n",
            "Clases: ['altar' 'apse' 'bell_tower' 'column' 'dome_inner' 'dome_outer'\n",
            " 'flying_buttress' 'gargoyle' 'stained_glass' 'vault']\n",
            "Train shape:(7369, 128, 128, 3)\n",
            "Test shape:(1843, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_s74gw3RuK6",
        "colab_type": "text"
      },
      "source": [
        "Primero antes de resolver el problema, hay que decidir que arquitectura utilizar para el problema de clasificación de imagenes. Entre las opciones más conocidas se encuentran: VGG16, VGG19, Resnet, entre otras arquitecturas.\n",
        "\n",
        "Una de las desventajas de las arquitecturas VGGXX es la cantidad de parametros a entrenar en el modelo en contraste a la profundidad de estos, en cambio las arquitecturas Resnet disminuye la cantidad de parámetros debido a la utilización de skip-connections, esto hace que estos modelos tengan una mayor profundidad. Debido a estas consideraciones se decidio utilizar una arquitectura resnet para abordar este problema.\n",
        "\n",
        "Puesto que se tienen una cantidad acotada de datos (aprox ~ 10.000 imagenes), se decide utilizar la técnica de transfer learning de un modelo resnet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTNlbN9Xdnko",
        "colab_type": "text"
      },
      "source": [
        "# Resnet 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eylVNG5Bs3An",
        "colab_type": "code",
        "outputId": "89c355fa-0a57-4703-99aa-13f0383c7e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.applications import ResNet152V2, ResNet50\n",
        "from keras.optimizers import Adam, SGD, Adagrad\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, UpSampling2D, GlobalAveragePooling2D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "input_shape = (width, height, depth)\n",
        "#newInput = Input(shape=(width, height, depth))\n",
        "resnet_model = ResNet50(include_top=False,\n",
        "                     weights=\"imagenet\",\n",
        "                     input_shape=input_shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(resnet_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(.25))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "for layer in resnet_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 8s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Up9lqDT4FA",
        "colab_type": "text"
      },
      "source": [
        "En primera instancia se utilizo un modelo preetrenado Resnet con una profundidad de 50 capas, en este modelo se quitaron las capas densas del final del modelo, para agregar las siguientes capas:\n",
        "\n",
        "1.   Average Pooling\n",
        "2.   Dense (250 neuronas) con función de activación relu\n",
        "3.   Dropout \n",
        "4.   Batch Normalizartion\n",
        "5.   Softmax\n",
        "\n",
        "Para un mejor desempeño de la red tambien se dejaron como entrenables las capas de batch que se encuentran entre las capas convolucionales del modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOkkOPmE2BqV",
        "colab_type": "text"
      },
      "source": [
        "## Resnet 152"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HzhBFv4yPBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "from keras.applications import ResNet152V2, ResNet50\n",
        "from keras.optimizers import Adam, SGD, Adagrad\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, UpSampling2D, GlobalAveragePooling2D, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "input_shape = (width, height, depth)\n",
        "#newInput = Input(shape=(width, height, depth))\n",
        "resnet_model = ResNet152V2(include_top=False,\n",
        "                     weights=\"imagenet\",\n",
        "                     input_shape=input_shape)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(resnet_model)\n",
        "model2.add(GlobalAveragePooling2D())\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dropout(.25))\n",
        "#model2.add(BatchNormalization())\n",
        "model2.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "for layer in resnet_model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p9qqtehVhOG",
        "colab_type": "text"
      },
      "source": [
        "En segunda instancia se hacen los mismos procedimientos pero en este caso se busca la mejora de resultados mediante el aumento de la profundidad del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1wNnwjEXYgk",
        "colab_type": "text"
      },
      "source": [
        "## Augmented Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoimVkw6VyPJ",
        "colab_type": "text"
      },
      "source": [
        "En un principio se entrenaron los dos modelos anterior directamente  con los datos de entrenamiento, sin embargo, se observaba overfitting, es por esto que se busca mejorar este aspecto mediante la técnica Data Augmentation, la cual consiste en aplicar un conjunto de operaciones a las imagenes del conjunto de entrenamiento para que la red reciba más datos de entrenamiento pero que ahora en teoria para la imagen no son las mismas imagenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ5j6V0eWtRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "test_datagen = ImageDataGenerator()\n",
        "train_datagen.fit(X_train)\n",
        "train_generator = train_datagen.flow(X_train,\n",
        "                                     y_train,\n",
        "                                     batch_size=32\n",
        "                                    )\n",
        "test_generator = test_datagen.flow(X_test,\n",
        "                                   y_test,\n",
        "                                   batch_size=32\n",
        "                                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K9AT9jCWn0f",
        "colab_type": "text"
      },
      "source": [
        "Para para el entrenamiento se propone un scheduler para el descenso de la tasa de aprendizaje, epecificamente se propone un decaimiento de la tasa de apredizaje por pasos, es decir, cada cierta cantidad de epocas la tasa de apredizaje decae en un porcentaje. En especifico, en este problema se entreno con una tasa de 0.01, disminuyendola a la mitad cada 5 epocas. \n",
        "\n",
        "Se escogio una cantidad pequeña de epocas para el decaimiento debido a la utilización de un modelo preentrenado, por esto el modelo no debe diversificar mucho en el espacio de soluciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89eux264X3y1",
        "colab_type": "code",
        "outputId": "5f569fa8-7cf9-49a3-9f46-20422240dc04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def step_decay(epoch):\n",
        "   initial_lrate = 0.01\n",
        "   drop = 0.5\n",
        "   epochs_drop = 5\n",
        "   lrate = initial_lrate * math.pow(drop,  \n",
        "           math.floor((1+epoch)/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath='final_model.h5', \n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "lr_scheduler = LearningRateScheduler(step_decay)\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "\n",
        "optimizer = Adam()\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "model2.summary()\n",
        "\n",
        "\n",
        "model2.fit(train_generator,\n",
        "                steps_per_epoch=len(X_train)//32,\n",
        "                epochs=50,\n",
        "                validation_data=test_generator,\n",
        "                callbacks=callbacks\n",
        "               )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet152v2 (Model)          (None, 4, 4, 2048)        58331648  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 58,858,762\n",
            "Trainable params: 670,858\n",
            "Non-trainable params: 58,187,904\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "230/230 [==============================] - 97s 420ms/step - loss: 0.8194 - accuracy: 0.7688 - val_loss: 0.5086 - val_accuracy: 0.8823\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88226, saving model to final_model.h5\n",
            "Epoch 2/50\n",
            "230/230 [==============================] - 66s 285ms/step - loss: 0.4369 - accuracy: 0.8668 - val_loss: 0.8389 - val_accuracy: 0.9208\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88226 to 0.92078, saving model to final_model.h5\n",
            "Epoch 3/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.3633 - accuracy: 0.8865 - val_loss: 0.2633 - val_accuracy: 0.9143\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.92078\n",
            "Epoch 4/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.3190 - accuracy: 0.9035 - val_loss: 0.5706 - val_accuracy: 0.9273\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.92078 to 0.92729, saving model to final_model.h5\n",
            "Epoch 5/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.3088 - accuracy: 0.9081 - val_loss: 0.2455 - val_accuracy: 0.9143\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.92729\n",
            "Epoch 6/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2920 - accuracy: 0.9109 - val_loss: 0.0174 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.92729\n",
            "Epoch 7/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2531 - accuracy: 0.9242 - val_loss: 0.5827 - val_accuracy: 0.9371\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.92729 to 0.93706, saving model to final_model.h5\n",
            "Epoch 8/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.2412 - accuracy: 0.9254 - val_loss: 0.0277 - val_accuracy: 0.9316\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93706\n",
            "Epoch 9/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.2477 - accuracy: 0.9230 - val_loss: 0.7496 - val_accuracy: 0.9392\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.93706 to 0.93923, saving model to final_model.h5\n",
            "Epoch 10/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2560 - accuracy: 0.9256 - val_loss: 0.1001 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93923\n",
            "Epoch 11/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2351 - accuracy: 0.9274 - val_loss: 0.0631 - val_accuracy: 0.9392\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93923\n",
            "Epoch 12/50\n",
            "230/230 [==============================] - 65s 285ms/step - loss: 0.2222 - accuracy: 0.9298 - val_loss: 0.1691 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93923\n",
            "Epoch 13/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2276 - accuracy: 0.9298 - val_loss: 0.1773 - val_accuracy: 0.9392\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93923\n",
            "Epoch 14/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.2119 - accuracy: 0.9370 - val_loss: 0.0990 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93923\n",
            "Epoch 15/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2046 - accuracy: 0.9365 - val_loss: 2.0793 - val_accuracy: 0.9387\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93923\n",
            "Epoch 16/50\n",
            "230/230 [==============================] - 65s 281ms/step - loss: 0.2008 - accuracy: 0.9366 - val_loss: 0.9858 - val_accuracy: 0.9376\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93923\n",
            "Epoch 17/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.2026 - accuracy: 0.9372 - val_loss: 0.3169 - val_accuracy: 0.9267\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93923\n",
            "Epoch 18/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.2194 - accuracy: 0.9357 - val_loss: 0.1003 - val_accuracy: 0.9414\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.93923 to 0.94140, saving model to final_model.h5\n",
            "Epoch 19/50\n",
            "230/230 [==============================] - 65s 285ms/step - loss: 0.2028 - accuracy: 0.9374 - val_loss: 0.1002 - val_accuracy: 0.9360\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.94140\n",
            "Epoch 20/50\n",
            "230/230 [==============================] - 66s 286ms/step - loss: 0.1695 - accuracy: 0.9467 - val_loss: 0.2322 - val_accuracy: 0.9425\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.94140 to 0.94249, saving model to final_model.h5\n",
            "Epoch 21/50\n",
            "230/230 [==============================] - 66s 286ms/step - loss: 0.1371 - accuracy: 0.9565 - val_loss: 0.1486 - val_accuracy: 0.9512\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.94249 to 0.95117, saving model to final_model.h5\n",
            "Epoch 22/50\n",
            "230/230 [==============================] - 66s 285ms/step - loss: 0.1297 - accuracy: 0.9613 - val_loss: 0.2792 - val_accuracy: 0.9566\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.95117 to 0.95659, saving model to final_model.h5\n",
            "Epoch 23/50\n",
            "230/230 [==============================] - 66s 285ms/step - loss: 0.1358 - accuracy: 0.9584 - val_loss: 0.0196 - val_accuracy: 0.9474\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.95659\n",
            "Epoch 24/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.1275 - accuracy: 0.9591 - val_loss: 0.1215 - val_accuracy: 0.9436\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.95659\n",
            "Epoch 25/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1358 - accuracy: 0.9565 - val_loss: 0.4851 - val_accuracy: 0.9430\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.95659\n",
            "Epoch 26/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1184 - accuracy: 0.9601 - val_loss: 0.0849 - val_accuracy: 0.9457\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.95659\n",
            "Epoch 27/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1111 - accuracy: 0.9657 - val_loss: 0.1106 - val_accuracy: 0.9609\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.95659 to 0.96093, saving model to final_model.h5\n",
            "Epoch 28/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.1099 - accuracy: 0.9624 - val_loss: 0.1363 - val_accuracy: 0.9528\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.96093\n",
            "Epoch 29/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1087 - accuracy: 0.9637 - val_loss: 0.4945 - val_accuracy: 0.9506\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.96093\n",
            "Epoch 30/50\n",
            "230/230 [==============================] - 65s 281ms/step - loss: 0.1212 - accuracy: 0.9601 - val_loss: 0.0495 - val_accuracy: 0.9490\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.96093\n",
            "Epoch 31/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1170 - accuracy: 0.9622 - val_loss: 0.0276 - val_accuracy: 0.9479\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.96093\n",
            "Epoch 32/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.1118 - accuracy: 0.9646 - val_loss: 0.1593 - val_accuracy: 0.9452\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.96093\n",
            "Epoch 33/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1293 - accuracy: 0.9632 - val_loss: 0.0050 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.96093\n",
            "Epoch 34/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.1131 - accuracy: 0.9647 - val_loss: 0.1177 - val_accuracy: 0.9463\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.96093\n",
            "Epoch 35/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.1163 - accuracy: 0.9659 - val_loss: 0.0114 - val_accuracy: 0.9566\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.96093\n",
            "Epoch 36/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.1029 - accuracy: 0.9654 - val_loss: 6.0208e-04 - val_accuracy: 0.9533\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.96093\n",
            "Epoch 37/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.1085 - accuracy: 0.9663 - val_loss: 0.0601 - val_accuracy: 0.9528\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.96093\n",
            "Epoch 38/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.1127 - accuracy: 0.9633 - val_loss: 0.0816 - val_accuracy: 0.9506\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.96093\n",
            "Epoch 39/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.0916 - accuracy: 0.9689 - val_loss: 0.0797 - val_accuracy: 0.9425\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.96093\n",
            "Epoch 40/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.0919 - accuracy: 0.9704 - val_loss: 8.9279e-06 - val_accuracy: 0.9588\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.96093\n",
            "Epoch 41/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.0913 - accuracy: 0.9698 - val_loss: 0.0013 - val_accuracy: 0.9588\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.96093\n",
            "Epoch 42/50\n",
            "230/230 [==============================] - 65s 281ms/step - loss: 0.0815 - accuracy: 0.9731 - val_loss: 0.0781 - val_accuracy: 0.9528\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.96093\n",
            "Epoch 43/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.0883 - accuracy: 0.9718 - val_loss: 0.0040 - val_accuracy: 0.9593\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.96093\n",
            "Epoch 44/50\n",
            "230/230 [==============================] - 65s 282ms/step - loss: 0.0729 - accuracy: 0.9781 - val_loss: 1.0169 - val_accuracy: 0.9544\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.96093\n",
            "Epoch 45/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.0650 - accuracy: 0.9779 - val_loss: 0.6029 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.96093\n",
            "Epoch 46/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.0805 - accuracy: 0.9734 - val_loss: 7.3276e-04 - val_accuracy: 0.9544\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.96093\n",
            "Epoch 47/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.1422 - val_accuracy: 0.9539\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.96093\n",
            "Epoch 48/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.0779 - accuracy: 0.9726 - val_loss: 0.0346 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.96093\n",
            "Epoch 49/50\n",
            "230/230 [==============================] - 65s 283ms/step - loss: 0.0670 - accuracy: 0.9784 - val_loss: 0.4529 - val_accuracy: 0.9555\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.96093\n",
            "Epoch 50/50\n",
            "230/230 [==============================] - 65s 284ms/step - loss: 0.0755 - accuracy: 0.9768 - val_loss: 0.6277 - val_accuracy: 0.9560\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.96093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f9cbd8abdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65pj9sIjX44B",
        "colab_type": "text"
      },
      "source": [
        "A partir de todo esto se escoge el modelo o movimiento dentro del espacio de soluciones que tiene mejor accuracy en el conjunto de test en las 100 epocas de entrenamiento, para evitar el overfitting. Con esto se llego a un accuracy cercano 96% en el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5WIRbG0AS4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48fafc4e-f623-422b-8ea3-64e3ee2ee924"
      },
      "source": [
        "from keras.models import load_model\n",
        "resnetModel = load_model('/content/final_model.h5')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uytirGszPZjU",
        "colab_type": "code",
        "outputId": "f4d5e7eb-1bb3-4aba-ccce-641a8b84bf2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dir_test = '/content/test_images'\n",
        "extension='jpg'\n",
        "\n",
        "paths_test = []\n",
        "# print(listdir(dir_test))\n",
        "for f in listdir(dir_test):\n",
        "    if isfile(join(dir_test,f)):\n",
        "        path = join(dir_test,f)\n",
        "        number = f.split('_')[1]\n",
        "        number = int(number.split('.')[0])\n",
        "        paths_test.append((number, path)) \n",
        "paths_test.sort(key= lambda file: file[0])\n",
        "\n",
        "imgs_test = []\n",
        "for _, path in paths_test:\n",
        "    image_to_predict = Image.open(path).convert(\"RGB\")\n",
        "    image_to_predict=np.asarray(image_to_predict)\n",
        "    imgs_test.append(image_to_predict)\n",
        "\n",
        "X_kaggle= np.array(imgs_test)/127.5 - 1\n",
        "X_kaggle.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1023, 128, 128, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XszinCfIUMmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_kaggle = resnetModel.predict(X_kaggle)\n",
        "y_kaggle = y_kaggle.argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3LK8Mf7UZh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inv_labels = {v: k for k, v in labels.items()}\n",
        "\n",
        "ids = [f'test_{i[0]}' for i in paths_test]\n",
        "\n",
        "data_kaggle = pd.DataFrame({'Id': ids , 'Expected':y_kaggle})\n",
        "data_kaggle['Expected'] = data_kaggle['Expected'].replace(inv_labels)\n",
        "data_kaggle.to_csv('9no_intento.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}